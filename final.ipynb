{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.exposure import histogram\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset=[]\n",
    "histSet=[]\n",
    "trainingsetHist=[]\n",
    "testsetHist=[]\n",
    "testset=np.array([])\n",
    "labels=np.array([])\n",
    "time_file = open(\"time.txt\", 'a')\n",
    "result_file = open(\"results.txt\", 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(images,label,isTrain=0,numPoints=11,radius=10, eps=1e-7):\n",
    "    global trainingsetHist\n",
    "    global testsetHist\n",
    "    for image in images:\n",
    "        lbp = local_binary_pattern(image, numPoints, radius, method= \"uniform\" ) \n",
    "        (hist, _) = np.histogram(lbp.ravel(),bins=np.arange(0, numPoints + 3),range=(0, numPoints + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    "        if isTrain==0:\n",
    "            trainingsetHist.append([hist,label])\n",
    "        else:\n",
    "            testsetHist.append(hist)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutLines(gray):\n",
    "    edges = cv2.Canny(gray,90,100,apertureSize = 3)\n",
    "    minLineLength=55\n",
    "    lines = cv2.HoughLinesP(image=edges,rho=1,theta=np.pi/180, threshold=100,lines=np.array([]), minLineLength=minLineLength,maxLineGap=0)\n",
    " \n",
    "    linesNew=[ line[0][1] for line in lines if line[0][1]==line[0][3]]\n",
    "    linesNew=sorted(linesNew)\n",
    "    finalList=[linesNew[0]]\n",
    "    for i in range(1,len(linesNew)):\n",
    "        if linesNew[i]-linesNew[i-1]>=100:\n",
    "            finalList.append(linesNew[i])\n",
    "    if len(finalList)>=3 and finalList[1]>=300 and finalList[1]<=800 and finalList[2]>=2000 and finalList[2]<=3000 :\n",
    "        return gray[finalList[1]:finalList[2],:]\n",
    "    else:\n",
    "        return gray[700:2200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_segments(gray):\n",
    "    th, im_th_otsu = cv2.threshold(gray, 128, 192, cv2.THRESH_OTSU)\n",
    "    ret,thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY_INV)\n",
    "    kernel = np.ones((10,200), np.uint8)\n",
    "    img_dilation = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    ctrs, hier = cv2.findContours(img_dilation.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    images = [im_th_otsu[y:y+h, x:x+w] for x,y,w,h in [cv2.boundingRect(ctr) for ctr in ctrs] if(len(sum(im_th_otsu[y:y+h, x:x+w]))>200 and h>40)]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(pathToData='new data'):\n",
    "    global trainingset\n",
    "    global labels\n",
    "    global histSet\n",
    "    global trainingsetHist\n",
    "    global testsetHist\n",
    "    global time_file\n",
    "    global result_file\n",
    "        \n",
    "    data= sorted(os.listdir(pathToData),key=int)\n",
    "\n",
    "    path=pathToData\n",
    "    testset=None\n",
    "    for datafolder in data:\n",
    "        path+='/'+datafolder\n",
    "        imageFolders=os.listdir(path)\n",
    "        for imageFolder in sorted(imageFolders):\n",
    "            if(imageFolder=='test.png'):\n",
    "                testset=cv2.imread(path+'/'+imageFolder,cv2.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                trainingset.append([cv2.imread(path+'/'+imageFolder+'/1.png',cv2.IMREAD_GRAYSCALE),int(imageFolder)])\n",
    "                trainingset.append([cv2.imread(path+'/'+imageFolder+'/2.png',cv2.IMREAD_GRAYSCALE),int(imageFolder)])\n",
    "        path=pathToData\n",
    "        start = time.time()\n",
    "        for image in trainingset:\n",
    "            newImage=cutLines(image[0])\n",
    "            segments=lines_segments(newImage)\n",
    "            describe(segments,image[1])\n",
    "        random.shuffle(trainingsetHist)\n",
    "        for datatrain in trainingsetHist:\n",
    "            histSet.append(datatrain[0])\n",
    "            labels=np.append(labels,datatrain[1])\n",
    "\n",
    "        #------------models-----------------------\n",
    "        #model = SVC(C=5.0, gamma='auto', probability=True)\n",
    "        #model= AdaBoostClassifier(n_estimators=250, random_state=0)\n",
    "        #model=GaussianNB()\n",
    "        #model=MultinomialNB()\n",
    "        #model= KNeighborsClassifier(n_neighbors=1)\n",
    "        model=RandomForestClassifier(max_depth=100,n_estimators=100, random_state=0)\n",
    "        #model = BaggingClassifier(n_estimators=400,random_state=0)\n",
    "        #model=MLPClassifier(solver='adam',hidden_layer_sizes=(64, 8), random_state=0,max_iter=20000,learning_rate_init=0.0001)\n",
    "        #model= GradientBoostingClassifier(n_estimators=100, learning_rate=0.00001,max_depth=50, random_state=0)\n",
    "        #------------------------------------------\n",
    "        model.fit(histSet, labels)\n",
    "        newImage=cutLines(testset)\n",
    "        segments=lines_segments(newImage)\n",
    "        describe(segments,-1,1)\n",
    "        p = model.predict_proba(testsetHist)\n",
    "        p = np.sum(p, axis=0)\n",
    "        \n",
    "        y_pred = model.classes_[np.argmax(p)]\n",
    "        end = time.time()\n",
    "        result_file.write(str(int(y_pred)) +'\\n')\n",
    "        t = (end - start)\n",
    "        time_file.write(str(round(t,2)) + '\\n')\n",
    "        \n",
    "\n",
    "        trainingset=[]\n",
    "        trainingsetHist=[]\n",
    "        labels=np.array([])\n",
    "        histSet=[]\n",
    "        testsetHist=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global time_file\n",
    "    global result_file\n",
    "    path=str(input('please enter the path to the dataset folder '))\n",
    "    run(pathToData=path)\n",
    "    result_file.close()\n",
    "    time_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
